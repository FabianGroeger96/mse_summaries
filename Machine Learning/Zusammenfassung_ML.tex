\documentclass[11pt]{article}

\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage[a4paper,margin=1cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{isodate}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{tabularx}
\usepackage{ltablex} % Longtables with tabularx
\usepackage[x11names]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{scalerel}
\usepackage{stackengine}
\usepackage{mathtools}
\usepackage{pdfpages}

% Code highlighting
\usepackage{minted}
\surroundwithmdframed{minted}

% Be able to caption equations and float them in place
\usepackage{float}

\newmdtheoremenv{theorem}{Theorem}

\theoremstyle{definition}
\newmdtheoremenv{definition}{Definition}[section]


\geometry{a4paper, margin=2.4cm}

\newcommand\equalhat{\mathrel{\stackon[1.5pt]{=}{\stretchto{\scalerel*[\widthof{=}]{\wedge}{\rule{1ex}{3ex}}}{0.5ex}}}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\newcommand*\samplemean[1]{\overline{#1}}
\newcommand*\ev[1]{\mathrel{\text{E}\left[#1\right]}}
\newcommand*\R{\mathbb{R}}
\newcommand*\Z{\mathbb{Z}}
\newcommand*\N[1]{\mathcal{N}\left(#1\right)}
\newcommand*\Likelihood{\mathcal{L}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand*\Exp[1]{\mathop{\text{Exp}}\left(#1\right)}
\newcommand*\Cov[1]{\mathop{\text{Cov}}\left(#1\right)}
\newcommand*\Cor[1]{\mathop{\text{Cor}}\left(#1\right)}
\newcommand*\Var[1]{\mathop{\text{Var}}\left(#1\right)}

\newcommand{\incond}{\rotatebox[origin=c]{90}{$\vDash$}}
\newcommand{\dcond}{\rotatebox[origin=c]{90}{$\nvDash$}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\graphicspath{{./img/}}

\begin{document}
	
\title{Machine Learning FS20}
\author{Pascal Baumann\\pascal.baumann@stud.hslu.ch}
\maketitle



For errors or improvement raise an issue or make a pull request on the \href{https://github.com/KilnOfTheSecondFlame/mse_summaries}{github repository}.

\tableofcontents
\newpage



\section{Introduction}

\subsection{Inductive Learning}

Inductive learning has the goal to \textbf{discover general concepts} from a \textbf{limited set of examples}, a model using inductive reasoning obtains new general knowledge from specific information. Throughout the learning process it is not truth preserving, new information can invalidate the current model, and works heuristically. The assumption therefore is that a model fitted to a sufficiently large learning set will be able to generalise to unseen data.

\begin{landscape}
	\subsubsection{Machine Learning Flowchart}
	\begin{center}
		\includegraphics[height=0.9\textheight, keepaspectratio]{scikit-learn_algorithm_cheatsheet}
	\end{center}
\end{landscape}

\subsection{Inductive Supervised Learning}
The data usually has a semi-formal representation in attribute-label pairs $(\textbf{x}, y)$. The labels encode concepts or classes.
Approximate the mapping function from example $x$ to label $y$ with a hypothesis $h(x) = \hat{y} \approx f(x)$ so that it generalises well.

\subsection{Learning as Search}
\begin{center}
	\includegraphics[width=0.7\linewidth]{img/hypothesis_space}
\end{center}

The hypothesis space $\mathcal{H}$ contains all possible hypotheses that can be built with the chosen representation, learning can thus be understood as a search for the global minima in this space $\mathcal{H}$.

\noindent
Formally this goal is expressed as follows: Find the hypothesis $h^* (x,\theta) = \hat{y}$ that best fits the training data, according to a loss function $L(h(x,\theta))$ by searching the hypothesis space $\mathcal{H} = \left\{ h(x,\theta) \middle| \theta \in P\right\}$.

\subsection{Inductive Bias}
\begin{theorem}
	A learner that makes \textbf{no a-priori assumptions} regarding the identity of the target concept has \textbf{no rational basis for classifying} any unseen instances. - Mitchell
\end{theorem}
All learning algorithms have some preformed hypothesis that is helpful to look for. It is beneficial to choose algorithms whose implicit hypothesis fits the data.

No free lunch theorem regarding the general equivalence of learners states that if all functions $f$ are equally likely, the probability of observing an arbitrary sequence of cost values during training does not depend on the learning algorithm $\mathcal{L}$.

The inductive bias of a learning algorithm $\mathcal{L}$ for instances in $X$ are any \textbf{minimal set of assertions} $B$ that, together with $\mathcal{L}$ and the training set $D$ \textbf{allows for deductively inferring} the $y'$ for a new $x\in X$. Make all assumptions \textbf{explicit} in $B$ such that $\forall x' \in X: \left(B, \mathcal{L}, D, x' \right) \Rightarrow y'$ is provable.

Machine Learning depends on the intelligent choice of the class $\mathcal{H}$ where $\mathcal{L}$ optimises the parameters. Machine Learning algorithms can be categorised by the strength of their inductive bias.

\subsection{Inductive Unsupervised Learning}
\begin{minipage}{0.6\linewidth}
	The usual task is Clustering, where the data $D$ are described by feature vectors without any labels, interesting structures and relationships in the data are then searched. The data naturally fall into $K$ groups, where the number of classes are not determined at the start but found through the learning scheme. This presents the challenge of searching by similarity in \textbf{distance} or \textbf{density}, which is another inductive bias to be considered, and by the choice of \textbf{parameters}.
	
\end{minipage}
\begin{minipage}{0.4\linewidth}
	\begin{center}
		\includegraphics[width=0.6\linewidth]{img/clustering_example}
	\end{center}
\end{minipage}

\subsection{Learnability}
Any target function $f$ over an instance $X$ is learnable given an expressive enough deterministic hypothesis space $\mathcal{H}$, a large enough training set $D$, and stationarity of the distribution $X$, that is instances in $D_{\text{train}}$ and $D_{\text{test}}$ are independently, identically distributed.

Better questions are what size of \textbf{$D_{\text{train}}$} are large enough and given a large enough training set, how well does the \textbf{training error predict generalisability}. These questions are in the domain of \emph{computational learning theory}.

\subsection{Probably Approximate Correct Learning and Vapnik-Chervonenkis Complexity}
Sample complexity bounds using $\abs{\mathcal{H}}$ usually do substantially overestimate, thus the assumption that $\bm{f} \in \mathcal{H}$ is unrealistic.

Measuring Vapnik-Chervonenkis (VC) dimension for infinite $\mathcal{H}$:
$h\in\mathcal{H}$ is \textbf{shattering a set of instances} $S\in X$ \textbf{iif} $h$ can partition $S$ in any way possible.\\
$VC(\mathcal{H}) := \abs{\{S\in X | S \text{ is the largest subset of } X \text{ shattered by any } h\in\mathcal{H}\}}$. $VC(\mathcal{H})$ can be used as an alternative measure of $\abs{\mathcal{H}}$ to compute sample complexity.

\begin{figure}[H]
	\centering
	\includegraphics[keepaspectratio,width=0.9\linewidth]{VC_shattering}
	\caption{A two-dimensional linear classifier can shatter three points, $VC(\text{two-dimensional straight lines}=3$ but $\abs{\mathcal{H}} = \infty$}
\end{figure}

\section{Formulating Learning Problems}
\subsection{Designing a Learning Solution}
\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{designing_learning_solution.pdf}
\end{center}

Designing a problem that is easiest for the algorithm to solve is the best thing one could do while formulating the problem. The first thing to do while first analysing data is to plot it.

\subsection{Machine Learning Development Process - Exploration and Experimentation}
There is a necessity to have a distinct conceptual approach to model data. Focus on \textbf{systematic experimentation} and \textbf{rigorous evaluation}, even automatised if needed. This is best implemented by a \textbf{pipeline of scripts} similar to the UNIX command line approach. Data exploration and rapid prototyping is key in the beginning of this process.

\begin{center}
	\includegraphics[width=0.6\linewidth]{exploration_evaluation_pipeline}
\end{center}

\section{Machine Learning Guiding Principles}

\subsection{Cost Function J}
Choose $\theta_0, \theta_1$ in such a way that $h(x)$ is close to $y$ for the training examples $(x,y)$. Let a function $J$ \textbf{number the cost of errors made} for the specific set of parameters.

\begin{equation*}
	J_{MSE}(\theta_0, \theta_1) = \frac{1}{2N} \sum_{i=1}^{N} \left(h(x_i, \vec{\theta}) - y_i\right)^2
\end{equation*}

The objective is to minimise $J$ with regards to $\theta_0, \theta_1$

\subsection{Optimisation by Gradient Descent}
Algorithm
\begin{itemize}
	\item Start with a random $\theta_0, \theta_1$
	\item Keep changing $\theta_0, \theta_1$ in a way to reduce $J(\theta_0, \theta_1)$
	\item End at a local minimum
\end{itemize}

\begin{align*}
	\text{\texttt{tmp\_0}} &:= \theta_0 - \alpha \cdot \frac{\partial J}{\partial \theta_0} J(\theta_0,\theta_1)\\
	\text{\texttt{tmp\_1}} &:= \theta_1 - \alpha \cdot \frac{\partial J}{\partial \theta_1} J(\theta_0,\theta_1)\\
	\theta_0 &:= \text{\texttt{tmp\_0}}\\
	\theta_1 &:= \text{\texttt{tmp\_1}}\\
\end{align*}

$\alpha$ is called the learning rate, and is a data-dependent hyper-parameter of the algorithm.

\section{Model Assessment and Selection}

\subsection{Data Handling for Model Evaluation}
Search for methods that help to learn and evaluate algorithms based on \emph{limited data} and help to \emph{deduce the true error} from the training error.

\begin{itemize}[leftmargin=*, labelindent=3cm, labelsep=1cm]
	\item[Model Assessment] \textbf{Evaluate} a model's \textbf{performance}
	\item[Model Selection] \textbf{Select} among competing models the one with the \textbf{proper level of flexibility}
\end{itemize}

\begin{center}
	\includegraphics[width=0.9\linewidth]{img/model_flexibility}
\end{center}

\subsection{Optimal Usage of a Small Dataset}
Split into Training, Validation and Test set. Do k-fold Cross Validation on Training and Validation set, final test on Test set. Train k times on $(l - 1)$ folds, validate on the remaining one (until each fold was used for validation once), then average the error.

\subsection{Observable and Unobservable Errors}
True error $E_D$ is the probability that $h$ will misclassify a random instance from the \textbf{complete domain} $\bm{E}$ and is unobservable.

The empirical test error $E_{emp}$ is the proportion of \textbf{sample $S\in D$} misclassified by $h$ is en estimate for the true error and gets better with more error.

\begin{itemize}
	\item Assumption that training and test data are representative of underlying distribution of $D$
	\item $S$ and $h$ are usually not chosen independently, that means that the test error is \textbf{optimistically biased}
	\item The test error usually varies for different $S\in D$, that means it has a higher variance than the true error
\end{itemize}

\subsection{Error Sources}

The chosen hypothesis $\mathcal{H}$ is the \textbf{best hypothesis at a distance of the true function}. Different chosen samples $X$ give \textbf{different information}.

\noindent
Error decomposition:
\begin{equation*}
	E_{MSE} = \underbrace{\text{systematic error}}_{\text{bias}} + \underbrace{\text{dependence on specific sample}}_{\text{variance}}  + \underbrace{\text{random nature of process}}_{\text{irreducible noise or Bayes rate}}
\end{equation*}

\subsection{ROC Curve}
First used in signal detection to show trade-off between \textbf{hit rate} and \textbf{false alarm rate} over noisy channel

\begin{center}
	\includegraphics[width=0.7\linewidth]{img/ROC_curve}
\end{center}

\noindent
\textbf{Interpretation}
\begin{itemize}[leftmargin=*, labelindent=3cm, labelsep=1cm]
	\item[Straight Line] Indicates a random process
	\item[Jagged Curve] Created with one set of test data
	\item[Smooth Curve] Created using averages
\end{itemize}

\subsection{Error Measures}
\begin{tabularx}{\linewidth}{lX}
	Mean Square Error & $ E_{MSE} = \frac{1}{F}\sum_{i=1}^{N} (\widehat{y}_i - y_i)^2 $\\
	Root Mean-Squared Error & $E_{RMSE} = \sqrt{\frac{1}{F}\sum_{i=1}^{N} (\widehat{y}_i - y_i)^2 } $\\
	Mean Absolute Error & $ E_{MAE} = \frac{1}{F}\sum_{i=1}^{N} \abs{\widehat{y}_i - y_i}$\\
	(less sensitive to outliers)
\end{tabularx}

\subsection{Evaluating Clustering Methods}
\textbf{Without Labels}:\\
Use the \textbf{silhouette coefficient}, which is a measure for cluster validity or consistency, do a visual inspection of dendrograms and a visual comparison of the dimension reduction o the feature vectors.

\vspace{1em}
\noindent
\textbf{With ground truth available}
Check \textbf{purity}, \textbf{rand index} and \textbf{misclassification rate}.

\subsection{Selection Among Competing Models}
On what basis can one \emph{algorithm be favoured over another} and how probable is it that the chosen method is \emph{truly significantly better}.

\subsection{Maximum Likelihood and Ockham's Razor}
Given competing $h_i \in \mathcal{H}_j$ \textbf{maximum likelihood parameters} can be found by \textbf{calculating the likelihood} $p(X|h_i)$ and \textbf{selecting the best} $\hat{h} = \underset{h_i}{\max} p(X|h_i)$.

The goal is to find a compromise between model complexity and accuracy on the validation data. Ockham's razor, an axiom in machine learning, states that "given two models with the \textbf{same empirical error}, the simpler one should be preferred because \textbf{simplicity is desirable in itself}". The reasoning is that for a simple hypothesis the \textbf{probability of it having unnecessary conditions is reduced}.

\subsection{Determining True Best Classifier}
In practice tenfold cross validation is often good enough and it does not matter which classifier is really better. Otherwise the \textbf{Student's t-test} can be used to compare two samples. Generally, the difference between $\mu_{\mathcal{L}_A}$ and $\mu_{\mathcal{L}_A}$ of the obtained cross-validation error estimates follows a Student's distribution with $m-1$ degrees of freedom.

If the number of features $p$ is large in comparison to the number of instances $N$ use \textbf{boosting} or \textbf{SVM}. If the data set is \emph{severely imbalanced}, \textbf{non-standard loss functions} that take class distribution in account or \textbf{Bayesian methods} and appropriate prior probabilities should be considered.

\section{Support Vector Machines}

Each observation is a vector of values ($p$-dimensional) for which the SVM \textbf{constructs a hyperplane} to separate class members. There are different support vectors for different tasks
\begin{enumerate}
	\item Maximal margin \textbf{hyperplane} classifier, for linearly separable data
	\item Support vector \textbf{classifier}, for almost linearly separable data
	\item Support vector \textbf{machine}, for non-linearly separable data
\end{enumerate}

\subsection{Maximal Margin Classifier}
\begin{minipage}{0.6\linewidth}
	Chooses the hyperplane maximising the distance from the hyperplane to the closest training point, denoted the support vectors, and can be represented as a linear combination of only a few training points.
\end{minipage}
\begin{minipage}{0.4\linewidth}
	\centering
	\includegraphics[keepaspectratio,width=0.8\linewidth]{img/maximum_margin_classifier.png}
\end{minipage}

\begin{definition}
	Definition of a hyperplane
	\begin{equation*}
		\{ \vec{x}: \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p = 0 \}
	\end{equation*}
\end{definition}
Separating hyperrplanes for classes encoded $\pm 1$
\begin{align*}
	\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} &> 0 \qquad \text{ if } y_i = 1\\
	\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} &< 0 \qquad \text{ if } y_i = -1\\
	\intertext{Equivalently}
	y_i\cdot (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) &> 0
\end{align*}
Evaluating the hyperplane formula for a point $x$ gives its distance to the hyperplane, if $\bm{\beta}$ is a normal vector.

\subsection{Formulation of Optimisation Problem}
Intuitive Optimisation
\begin{itemize}[noitemsep]
	\item $\underset{\beta_0,\beta_1,\dots,\beta_p}{\text{maximise}} M$
	\begin{itemize}
		\item subject to $\sum_{j=1}^{p}\beta_j^2 = 1$
		\item and $y_i\cdot (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M\qquad\forall i = 1..N$
	\end{itemize}
\end{itemize}
This can be reformulated using {\color{DodgerBlue2} Lagrange multipliers}
\begin{itemize}[noitemsep]
	\item Find $L_D = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N}\left( \sum_{k=1}^{N} \alpha_i \alpha_k \bm{x}_i^T \bm{x}_k \right) $
		\begin{itemize}
		\item subject to $ \alpha_i \geq 0 $ and $\sum_{i=1}^{N} \alpha_i y_i = 0$
		\item Once the alphas are computed $\beta = \sum_{i=1}^{N} \alpha_i y_i \bm{x}_i$
	\end{itemize}
\end{itemize}

\subsection{Support Vector Classifier}
Based on the assumption that the data is not perfectly linearly separable any more, the idea is to introduce soft margins that allow for \emph{some} misclassifications. The number of misclassifications is controlled by a {\color{DodgerBlue2} penalty factor} $C$.

\begin{center}
	\includegraphics[width=0.7\linewidth]{img/support_vector_classifier}
\end{center}

\subsubsection{Support Vector Classifier Optimisation}
\begin{minipage}{0.6\linewidth}
Intuitive Optimisation
\begin{itemize}[noitemsep,nosep]
	\item $\underset{\beta_0,\beta_1,\dots,\beta_p}{\text{maximise}} M$
	\begin{itemize}
		\item subject to $\sum_{j=1}^{p}\beta_j^2 = 1$
		\item and $y_i\cdot (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M(1 - \xi_i)   \qquad\forall i = 1..N$
		\item $\xi_i \geq 0, \sum_{i=1}^{N}\xi_i \leq \tilde{C}$
		\item where $\xi_i$ is a slack variable to allow instance $i$ to lie on the wrong side of the margin
		\item $\tilde{C}$ is the total budget for misclassifications
	\end{itemize}
\end{itemize}
Equivalent technical optimisation, so called "dual form"
\begin{itemize}[noitemsep,nosep]
	\item maximise\\
	$L_D = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N}\sum_{k=1}^{N} \alpha_i \alpha_k y_i y_k x_i^T x_k$
	\begin{itemize}
		\item subject to $0 \leq \alpha_i \leq C$ and $\sum_{i=1}^{N}\alpha_i y_i = 0$
		\item $\beta = \sum_{i=1}^{N} \alpha_i y_i x_i$ is minimised
	\end{itemize}
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.8\linewidth]{img/support_vector_classifier_penalty}
\end{minipage}

\subsection{Support Vector Machines}
A possible solution for linearly completely non-separable data is to map the data into a higher-dimensional space. In this space a separating hyperplane may be defined. 

\subsubsection{Variable Transformation}
The same idea applies, transform a non-linearly separable input function in $\R^n$ to $\R^{n+k}, k\geq 1$ where it may be linearly separable.

\begin{center}
	\includegraphics[keepaspectratio,width=0.6\linewidth]{variable_transformation.png}
\end{center}

The power of feature space transformation lies in the fact, that with an appropriately chosen feature space of sufficient dimensionality any consistent training set can be made separable.
\begin{equation*}
	\left\langle \begin{pmatrix}x_i\\x_i^2\end{pmatrix} \cdot\begin{pmatrix}x_{i'}\\x_{i'}^2\end{pmatrix} \right\rangle = x_i x_{i' } + x_i^2 x_{i' }^2 := K(x_i,x_{i'})
\end{equation*}
$K$ is known as a \textbf{kernel} function. It can be used to efficiently compute feature space transformations. Instead of calculating the inner product, the kernel $K(\cdot,\cdot)$ gets calculated. As soon as some $K$ is used, the resulting SVC becomes a SVM.

The following Kernels are common
\begin{itemize}[noitemsep]
	\item {\color{DodgerBlue2} Identity (inner product)}
	\begin{equation*}
		K(x_i,x_{i'}) = \sum_{j=1}^{p} x_{ij}x_{i'j}
	\end{equation*}
	this yields the standard support vector classifier
	\item {\color{DodgerBlue2} Polynomial of degree $d$}
	\begin{equation*}
		K(x_i,x_{i'}) = \left(1 + \sum_{j=1}^{p} x_{ij}x_{i'j}\right)^d
	\end{equation*}
	the polynomial kernel of degree $d = 1$ is just the identity kernel
	\item {\color{DodgerBlue2} Radial basis or Gaussian}
	\begin{equation*}
		K(x_i,x_{i'}) = e^{\left( -\gamma \sum_{j=1}^{p}(x_{ij} - x_{i'j})^2 \right)}
	\end{equation*}
	the Gaussian kernel computes a very local neighbourhood, with $\gamma$ being the hyper-parameter controlling the width of that neighbourhood. A low $\gamma$ yields a great width. The feature space spanned by the Gaussian kernel is implicit and infinite-dimensional.
\end{itemize}

\subsection{Multiclass Classification With Support Vector Machine}
With one versus rest or all classification, the binary classifier SVM can be adapted to choose between multiple classes.
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/support_vector_machine_multiclass}
\end{center}
Assume $k$ classes, learn all $\frac{k(k-1)}{2}$ pairwise comparisons and classify unknown instances by majority vote among all pairwise comparisons.

\section{Ensemble Methods}
The goal of ensembles is to increase performance by \textbf{combining multiple complementary classifiers}. The intuition behind it is to build different experts and let them decide by vote. This is {\color{Green3} very effective in practice} and has {\color{Green3}good theoretical guarantees}, but while being {\color{Green3} easy to implement} the result {\color{Firebrick3} lacks in transparency}, {\color{Firebrick3}interpretability} and a {\color{Firebrick3} compact representation}.

The formal problem description is: Given $T$ binary classification hypotheses $(h_1,\dots, h_T)$, find a combined classifier with better performance of the form
\begin{equation*}
	\hat{h}(x) = \text{sign}\left( \sum_{t=1}^{T} \alpha_t h_t (x) \right)
\end{equation*}
For regression the average is used.

Ensembles work because they average the error, in statistical terms the bias remains equal but the variance is reduced.

\subsection{AdaBoost}
\subsubsection{Boosting}
The general idea is to boost the performance of weak learners iteratively by making misclassified examples more important and then combining hypotheses. This way, each stage additively corrects shortcomings of previous stages by reweighing and majority voting on the result. The \textbf{Ada}ptive \textbf{Boost}ing algorithm works by
\begin{minted}[escapeinside=||,mathescape=true,fontsize=\small]{python}
initialize weights: |$w_i := \frac{1}{N}$|
for |$ t:=1..T $|
	|$ h_t $| := train decision stump on the |$ x_i $|, weighted by the |$ w_i $|
	|$ \varepsilon_t $| := |$ \frac{\sum_{i=1}^{N} w_i \cdot I(y_i\neq h_t(x_i) }{\sum_{i=1}^{N} w_i} $| # compute error, $ I() $ is the identity function
	|$ \alpha_t $| := |$ \log\left(\frac{1-\varepsilon_t}{\varepsilon_t} \right) $| # compute influence of weak learner
	|$ w_i $| := |$ w_i\cdot e^{\alpha_t\cdot I(y_i \neq h_t(x))} $| # increase weight by exp(Influence) in case of error
return |$ \hat{h} $| := |$ \text{sign}\left(\sum_{t=1}^T \alpha_t\cdot h_t(x)\right) $| # majority vote
\end{minted}

\section{Bias, Variance and Learning Curves}

\begin{center}
	\includegraphics[width=0.8\linewidth]{img/bias_variance_tradeoff}
\end{center}

\subsection{Polynomial Regression using sklearn.pipeline}
\begin{minted}[linenos]{python}
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

model = make_pipeline(PolynomialFeatures(degree),
LinearRegression())
model.fit(x[:, np.newaxis], y)
ax.plot(x_test, model.predict(x_test[:, np.newaxis]), '-b')
\end{minted}

\subsection{Bias-Variance-Decomposition}
When the relationship between the predictor variable $X$ and the response variable $Y$ is written as
\begin{equation*}
	Y=f(X) + \epsilon \qquad \epsilon\sim\N{0,\sigma_{\epsilon}}
\end{equation*}
Then the expected value of the quadratic error can be written as
\begin{equation*}
	\text{SE}(x) = \ev{(Y - \hat{f}(x))^2}
\end{equation*}
Transformed
\begin{equation*}
	\text{SE}(x) = \underbrace{\left(\ev{\hat{f}(x)} - f(x)\right)^2}_{\text{Bias}^2} + \underbrace{\ev{\left(\hat{f}(x) - \ev{\hat{f}(x)}\right)^2}}_{\text{Variance}} + \underbrace{\sigma_{\epsilon}^2}_{\text{irreducible error}}
\end{equation*}

\subsection{The No-Free-Lunch Theorem (Probably Approximately Correct Theory)}
\begin{theorem}
	For every learner, there exists a task on which it fails, even though that task can be successfully learned by another learner.
\end{theorem}
Intuitively, as in the real world a learner is never trained on the whole data that exists, but only a subset of that data.

\subsection{Cross-Validation}
\begin{center}
	\includegraphics[width=0.7\linewidth]{img/cross_validation_hyperparameter_tuning}
\end{center}
\subsubsection{Stratified k-Fold Cross Validation for Unequal Classes}
In cases where classes are unevenly distributed, {\color{DodgerBlue3} stratified k-fold cross-validation} reduces bias and variance of the estimation. The implementation takes care to ensure that the classes are distributed approximately equally in the subsets. This ensures that the individual subsets are representative of the class distribution in the training data set.

\begin{minted}[linenos,fontsize=\small]{python}
from sklearn.model_selection import validation_curve

degrees = np.arange(1, 14)
model = make_pipeline(PolynomialFeatures(), LinearRegression())
# The parameter to vary is the "degrees" on the pipeline step "polynomialfeatures"

train_scores, validation_scores = validation_curve(
	model, x[:, np.newaxis],
	y,
	param_name='polynomialfeatures__degree',
	param_range=degrees)
\end{minted}

\subsubsection{Cross-Validated Grid Search in Python}
A grid search consists of
\begin{itemize}[nosep,noitemsep]
	\item an \textbf{estimator}
	\item a \textbf{parameter space}
	\item a method for searching or \textbf{sampling} candidates;
	\item a \textbf{cross-validation scheme}
	\item a \textbf{score function}
\end{itemize}
\begin{minted}[linenos,fontsize=\small]{python}
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV

iris = datasets.load_iris()

parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svc = svm.SVC(gamma="scale")

clf = GridSearchCV(svc, parameters, cv=5)
clf.fit(iris.data, iris.target)

sorted(clf.cv_results_.keys())
\end{minted}

\subsection{Learning Curves}
\begin{center}
	\includegraphics[width=0.9\linewidth]{img/learning_curve_bias_variance}
\end{center}
A \textbf{high bias} points to a systemic error or underfit, choose another model to approximate the real function. A \textbf{high variance} points to instability or overfit, getting more data or use more regularisation.
\subsection{Regularisation}
Ridge-Regression ($L_2$-Regression) \textbf{distributes weight} across related features and has an analytic solution
\begin{equation*}
	R_{\text{Ridge}}(\theta) = \sum_{i=1}^{d} \theta_i^2
\end{equation*}
LASSO Regression ($L_1$-Regression) encourages sparsity by setting some weights to zero and is used to select \textbf{informative features}
\begin{equation*}
	R_{\text{LASSO}}(\theta) = \sum_{i=1}^d\abs{\theta_i}
\end{equation*}

\subsection{Reduction of the Model Complexity}
Increasing the complexity of a model usually increases its variance and lowers its bias. Conversely, a lower complexity of the model increases its bias and lowers its variance. Therefore, this is called equilibrium. Regularising the model, by restricting it in some manner, is one way to avoid overfitting: The less freedom the model has, the more difficult it becomes to overfit the data. For example, a polynomial model can easily be regularised by reducing the degree of the polynomial.

\subsubsection{Minimisation of the Loss Function with Regularisation}
\begin{align*}
	L(\theta) &= \sum_{i=1}^{N} \left( y_i - \hat{y}_i(\theta) \right)^2 + \lambda \norm{\theta}_R\\
	\norm{\theta}_R:\qquad \norm{\theta}_2^2 &= \sum_{i=1}^{n} (\theta_k)^2\qquad \text{Ridge-Regression}\\
	\norm{\theta}_1 &= \sum_{i=1}^{n} \abs{\theta_k}\qquad \text{LASSO-Regression}\\
\end{align*}
with $\lambda$ being the Regularisation hyperparameter. By assigning larger values to the hyper parameter $\lambda$, we increase the regularisation strength and reduce the weights of our model. LASSO is an approach that can lead to sparse models. Depending on the regularisation strength, certain weights can become zero, which means that LASSO can also function as a \textbf{monitored feature selection procedure}.

\section{System Design}
Machine Learning systems as a whole are pipelines composed of individual components, that can be developed collaboratively as a team.

\subsection{Ceiling Analysis}
\begin{enumerate}[noitemsep,nosep]
	\item {\color{Tomato2} Baseline}: Measure the performance of the complete pipeline
	\item Replace {\color{Goldenrod1} first component} with the ground truth (perfect result for this stage) and measure performance of the system
	\item Replace {\color{OliveDrab3} next component} with the ground truth and measure performance of the system
	\item \qquad $\vdots$
\end{enumerate}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{img/ceiling_analysis.png}
	\caption{Based on this analysis to improve Text Detection and Character Recognition makes sense, Character Segmentation only improves the overall performance by 1\% and is thus not worth the investment}
\end{figure}

\section{Feature Engineering}
A feature is an individual measurable property of a phenomenon being observed.

\subsubsection{Exploratory Data Analysis}
Whether collected by your workgroup or obtained from someone else, raw data is seldom ready for immediate analysis. Exploratory data analysis can help in discovering important anomalies, identifying limitations in the collection process, and to better inform subsequent goal oriented analysis.

Before generating features for the model, identify key properties of the data including structure, granularity, faithfulness, temporality and scope. This can be done by preparing, analysing and visualising the data using Histograms, QQ-Plots, Scatter-matrices and Heatmaps.

\subsubsection{Data Cleansing}
\begin{minted}{python}
# drop duplicates, drop not available
df.drop_duplicates()
df.dropna()

# replace the cells without any data in the column with the value 'None'
df = df.astype(object).where(pd.notnull(df),None)
weekday_counts = df.groupby('weekday').aggregate(sum)

# replace * in Name with empty string:
df['Name']=df['Name'].apply(lambda x:str(x).replace('*',''))

# convert dates to datetime, keeping only the month
df['MonthYear'] = pd.to_datetime(df['MonthYear'])
df['MonthYear'] = df['MonthYear'].apply(lambda x: x.date())
\end{minted}

\subsection{Feature Preparation}
\begin{itemize}
	\item \textbf{Data Cleaning}\\
	Homogenise missing values and different types of in the same feature, fix input errors, types, etc.
	\item \textbf{Aggregating / Pivoting}\\
	necessary when the entity to model is an aggregation from the provided data.
	\item \textbf{Imputation of missing values}\\
	Strategies: mean, median, mode, using a model
	\item \textbf{Binarisation}\\
	transform discrete or continuous numeric features into binary features
	\item  \textbf{Binning}\\
	Fixed width, adaptive quantile binning
	\item \textbf{Transformation}\\
	Compress the range of large numbers or expand the range of small numbers.
	\item \textbf{Scaling and normalisation}
	\item \textbf{Generate Interaction features}
\end{itemize}

\subsubsection{Box-Cox-Transformation}
\begin{minipage}{0.5\linewidth}
	The Box-Cox Transformation can be used to symmetrize positive numerical features
	\begin{equation*}
		\tilde{x}_i = \left\{ \begin{matrix}
		\frac{x_i^\lambda - 1}{\lambda} & \text{if }\lambda\neq 0\\
		\log(x_i) & \text{otherwise}
		\end{matrix} \right.
	\end{equation*}
\end{minipage}
\begin{minipage}{0.5\linewidth}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{img/boxcox_lambdas}
	\end{center}
\end{minipage}
\begin{minted}{python}
from sklearn.preprocessing import PowerTransformer

# select numeric features
weather_num = weather.select_dtypes(include =["float"]) #
# 'yeo-johnson' (default), works with positive and negative values
# 'box-cox', only works with strictly positive values
PowTrans = PowerTransformer(method='yeo-johnson')
# fit and transform data
weather_num_t = PowTrans.fit_transform(weather_num)
\end{minted}

\subsection{Feature Preparation}
\subsubsection{One-Hot Encoding}
\begin{minted}{python}
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
X = [['Male', 1], ['Female', 3], ['Female', 2]]
enc.fit(X)
\end{minted}

\subsubsection{Bin Counting}
\begin{minted}{python}
rnd = np.random.RandomState(42); X = rnd.uniform(-3, 3, size=100)
y = np.sin(X) + rnd.normal(size=len(X)) / 3
X = X.reshape(-1, 1)
enc = KBinsDiscretizer(n_bins=10, encode='onehot')
X_binned = enc.fit_transform(X)
\end{minted}

\subsubsection{Label Encoding}
\begin{minted}{python}
le = preprocessing.LabelEncoder()
le.fit(["paris", "paris", "tokyo", "amsterdam"])
list(le.classes_)
> ['amsterdam', 'paris', 'tokyo']
le.transform(["tokyo", "tokyo", "paris"])
> array([2, 2, 1])
\end{minted}

\subsubsection{Note on NLP}
This section is covered in 'Analysis of Text Data' in much more detail, and has thus been omitted.

\subsection{Feature Selection}
The reason to use feature selection is reducing the number of features, to reduce overfitting and improve the generalization of models, and to gain  a better understanding of the features and their relationship to the response variables.

\subsection{Feature Selection using Linear Models and Regularisation}
Utilise machine learning models for feature ranking as many have some inherent internal ranking feature. Even simple linear regression
models can work when the data is not very noisy and the features are independent.

To avoid a multicollinearity problem either L1 (Lasso) or L2 (Ridge) regularisation should be used.

\begin{minted}{python}
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_boston

boston = load_boston()
scaler = StandardScaler()
X = scaler.fit_transform(boston['data'])
Y = boston['target']
names = boston['feature_names']

lasso = Lasso(alpha=.3)
lasso.fit(X,Y)
\end{minted}
\texttt{Lasso model: -3.707 * LSTAT + 2.992 * RM + -1.757 * PTRATIO + -1.081 * DIS\\
	+ -0.7 * NOX + 0.631 * B + 0.54 * CHAS + -0.236 * CRIM + 0.081 * ZN\\
	+ {\color{red} -0.0 * INDUS} + {\color{red} -0.0 * AGE} + {\color{red}0.0 * RAD} + {\color{red}-0.0 * TAX}}
L2 or Ridge regularisation squares the coefficients in the penalty expression. This forces the coefficient values to be spread out more equally.

\subsection{Tree-based Methods for Feature Selection}
Random forest are popular methods due to their good accuracy, robustness and ease of use. Additionally, they provide a mean decrease purity and mean decrease accuracy.

\subsubsection{Information Gain}
\begin{equation*}
	\text{Entropy }H(T) = -\sum_{i=1}^{K} p_i\cdot\log_2(p_i)
\end{equation*}
Where $p_i$ represent the percentage of each class present in the child node that results from a split in the tree.
\begin{equation*}
	\text{Information Gain } \text{IG}(T,a) = H(T) - H(T|a)
\end{equation*}
\begin{equation*}
	\text{IG}(T,a) = -\sum_{i=1}^{K} p_i\cdot\log_2(p_i) - \sum_a p(a)\cdot\sum_{i=1}^{K} p(i|a)\cdot\log_2[p(i|a)]
\end{equation*}

\subsection{Recursive Feature Elimination (RFE)}
Given an external estimator that weighs features, RFE selects features by recursively considering smaller sets of features.

\section{Bayes' Theorem}
\subsection{Multivariate Gaussian Distribution}
Gaussian defined over a vector x of continuous variables in a D-dimensional space with mean vector $\mu$ and covariance matrix $\bm{\Sigma}$, where $ \abs{\bm{\Sigma}} $ is the determinant of $\bm{\Sigma}$.
\begin{equation*}
	p(\bm{x}) = \N{\bm{x}|\mu, \bm{\Sigma}} = \frac{1}{(2\pi)^{\frac{D}{2}} \norm{\bm{\Sigma}}^{\frac{1}{2}}} \exp\left\{ -\frac{1}{2} (\bm{x} - \mu)^T \bm{\Sigma}^{-1} (\bm{x} - \mu) \right\}
\end{equation*}
The quadratic form in the argument of the exponential is called \emph{Mahalanobis distance}
\begin{equation*}
	\bm{\Delta} = (\bm{x} - \mu)^T \bm{\Sigma}^{-1} (\bm{x} - \mu)
\end{equation*}
Without loss of generality, we can assume that $\bm{\Sigma}$ is symmetric with real eigenvalues and an orthonormal set of eigenvectors $\mu_i$.

% TODO Finish MGD

% TODO Conditional Gaussian Distributions

\subsection{Belief Networks or Bayesian Networks}
A belief network is a directed acyclic graph (DAG) in which each node has associated the conditional probability of the node given its parents. The joint distribution is obtained by taking the product of the conditional probabilities
\begin{center}
	\includegraphics[width=0.7\linewidth]{img/belief_network}
\end{center}
$ \bm{X} \incond \bm{Y} | \bm{Z}$ denotes that two sets of variables $\bm{X}$ and $\bm{Y}$ are independent of each other given the state of the set of variables $\bm{Z}$. This means that
\begin{align*}
	p(\bm{X},\bm{Y}|\bm{Z}) &= p(\bm{X}|\bm{Z})\cdotp(,\bm{Y}|\bm{Z})\\
	p(\bm{X}|\bm{Y},\bm{Z}) &= p(\bm{X}|\bm{Z})
\end{align*}
for all states of $\bm{X},\bm{Y},\bm{Z}$. In case the conditioning set is empty we may also write
\begin{equation*}
	\bm{X} \incond \bm{Y}
\end{equation*}
in which case $\bm{X}$ is unconditionally independent of $\bm{Y}$.
\begin{center}
	\includegraphics[width=0.7\linewidth]{img/conditional_independence}
\end{center}
In (d) the variables $A,B$ are conditionally dependent given $C$
\begin{equation*}
	P(A,B|C) \propto p(C|A,B)p(A)p(B)
\end{equation*}

\subsubsection{Collider}
A collider contains two or more incoming arrows along a chosen path.
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/conditional_independence_collider}
\end{center}

\subsection{Bayes' Theorem}
\begin{center}
	\includegraphics[width=0.6\linewidth]{img/bayes_theorem}
\end{center}

\section{Gaussian Processes}
Is referred to as the \textbf{infinite-dimensional extension of the multivariate normal distribution}. When we are working with Gaussian Processes the intuition is that we observe some finite-dimensional subset of infinite-dimensional data, and this finite subset follows a multivariate normal distribution, as would every finite subset.

Gaussian random variables are useful in machine learning and statistics for two main reasons.
\begin{enumerate}
	\item They are extremely common when modelling 'noise' in statistical algorithms
	\item Gaussian random variables are convenient for many analytical manipulations, because many of the integrals involving Gaussian distributions that arise in practice have \textbf{simple closed form solutions}
\end{enumerate}

\subsection{Bayesian Regression}
\begin{align*}
	\intertext{Parameter Posterior}
	p(\theta|\mathcal{D}) &= \frac{p(\mathcal{D}|\theta)\cdot p(\theta)}{\int_{\theta'} p(\mathcal{D}|\theta)\cdot p(\theta) \diff \theta'}
	\intertext{Posterior Predictive Distribution}
	p(y_{*}|x_{*}\mathcal{D}) &= \int_{\theta} p(x_{*}|y_{*},\theta)\cdot p(\theta|\mathcal{D})\diff\theta
\end{align*}





\end{document}
