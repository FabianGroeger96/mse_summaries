\documentclass[11pt]{article}

\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage[a4paper,margin=1cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{isodate}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{tabularx}
\usepackage{ltablex} % Longtables with tabularx
\usepackage[x11names]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{scalerel}
\usepackage{stackengine}
\usepackage{mathtools}
\usepackage{pdfpages}

% Code highlighting
\usepackage{minted}
\surroundwithmdframed{minted}

% Be able to caption equations and float them in place
\usepackage{float}

\newmdtheoremenv{theorem}{Theorem}
\geometry{a4paper, margin=2.4cm}
\newtheorem*{remark}{Remark}

\newcommand\equalhat{\mathrel{\stackon[1.5pt]{=}{\stretchto{\scalerel*[\widthof{=}]{\wedge}{\rule{1ex}{3ex}}}{0.5ex}}}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\graphicspath{{./img/}}

\begin{document}
	
\title{Analysis of Text Data FS20}
\author{Pascal Baumann\\pascal.baumann@stud.hslu.ch}
\maketitle



For errors or improvement raise an issue or make a pull request on the \href{https://github.com/KilnOfTheSecondFlame/mse_summaries}{github repository}.

\tableofcontents
\newpage

\section{Introduction}
Text analysis consist of a series of operations completed by one or more pieces of software on a sample of written human language, with the goal of extracting useful information.

Applications of text analysis include:
\begin{itemize}[noitemsep]
	\item Analysis
	\begin{itemize}
		\item spell checkers
		\item keyword extraction
		\item authorship attribution
		\item document retrieval
		\item text classification
		\item text mining
		\item sentiment analysis
		\item content-based recommendation
	\end{itemize}
	\item Text Analysis and Generation
	\begin{itemize}
		\item machine translation
		\item automatic question answering
		\item automatic summarisation
	\end{itemize}
	\item Text Generation
	\begin{itemize}
		\item database report generation
		\item weather forecast generation
	\end{itemize}
	\item Analysis, Generation and Interaction
	\begin{itemize}
		\item dialogue systems
		\item assistive technology for teaching or writing
	\end{itemize}
\end{itemize}

Analysis of Text Data lies at the intersection of Machine Learning, Computational Linguistics and Human-Computer Interaction. Artificial Intelligence is applied to human language, while human speech data is a related problem which makes use of different technologies, like signal processing.

\subsection{Basic Concepts in NLP}
Machine Learning is a powerful tool in NLP, thus the same vocabulary is used. For Text Analysis supervised learning is used mostly, where the labelled data is classified by human experts and is called the reference or ground truth.

Significance addresses the key question if the difference between the two systems is really due to the fact that one is better than the other or if it can be explained by randomness. This is easier to compute when cross-validation is used, and paired $t$-tests can be used to compare two systems. Performance scores vary depending on the data set it is difficult to predict actual performance on a data set of a different
nature than the test set, this is called the problem of portability.

\subsection{Key Jargon from Linguistic}
\begin{itemize}
	\item letter
	\item syllable
	\item morpheme: a meaningful morphological unit of a language that cannot be further divided
	\item word
	\item phrase: a group of words standing together as a conceptual unit
	\item clause: a group of words in a sentence that contains a subject and a subject
	\item sentence:
	\begin{itemize}
		\item simple sentence: one independent clause
		\item compound sentence: at least two independent clauses
		\item complex sentence: at least one independent clause and one or more dependent clauses
	\end{itemize}
	\item text
	\item corpus: collection of text, pl. corpora
	\item utterance: an uninterrupted chain of spoken or written language
	\item lexeme: basic abstract unit of meaning that roughly corresponds to a set of forms taken by a single root word - for example run, runs, ran and running are forms of the same lexeme
	\item lemma:  one form chosen by convention as the canonical form of a lexeme - go, goes, went, gone, going for the lexeme \emph{go}
\end{itemize}

\subsection{Utterance Decoding}
Decode its propositional content or logical form first and then combine them. Draw inference to solve ambiguities, guess what is left unsaid and, in general, maximise the likelihood given the context.

\subsection{Lexical Analysis}
The goal of lexical analysis is to understand word forms.
\begin{enumerate}
	\item tokenisation: breaking a text into word tokens
	\item lemmatisation: finding the base form of each word token or lemma
	\item Part of Speech tagging: finding the part of speech each word token corresponds to
	\item Named Entity Recognition:  identification of proper nouns of people, places, organizations
\end{enumerate}

\subsection{Syntactic Analysis}
\begin{itemize}
	\item sentence segmentation or splitting
	\item identifying phrases or chunking
	\item figuring out logical functions
	\item building parse trees or parsing
\end{itemize}

\subsubsection{Constituency Parsing}
With syntactic parsing the connection between words can be understood. Constituency parsing breaks a sentence into its basic building blocks.

\begin{center}
	\includegraphics[width=\linewidth]{img/parse_tree_syntactic_parsing}
\end{center}

\subsubsection{Dependency Parsing}
Dependency Parsing helps in understanding what depends on what. The assumption is that the sentence is centred around the verb and what comes first in a sentence is more important than any information that comes after. So anything of the sentence depends on that word or some lower-level information.

\begin{center}
	\includegraphics[width=\linewidth]{img/parse_tree_dependency_parsing}
\end{center}

\subsection{Understanding Meaning}
\subsubsection{Semantic Analysis}
\begin{itemize}
	\item word-level
	\begin{itemize}
		\item word sense disambiguation
		\item co-occurrence analysis
	\end{itemize}
	\item sentence-level or text-level
	\begin{itemize}
		\item semantic role labelling
		\item co-reference resolution
	\end{itemize}
\end{itemize}

\subsubsection{Discourse Analysis}
\begin{itemize}
	\item topics
	\item sentiments
	\item speech or dialogue acts
	\item argumentative structures
\end{itemize}

\subsection{Data for Text Analysis}
Text analysis using machine learning requires large amounts of training data and finding suitable data is often a bottleneck due to expense or limited rights. An annotated corpus typically contains a selection of texts based on explicit criteria, metadata like author, date, source, title, sectioning, and annotations in a more or less standardises format.

\subsection{Preprocessing}
Most text processing tasks begin with a set of standard preprocessing steps
\begin{itemize}
	\item Tokenisation: segmentation into tokens
	\item Token normalisation
	\item Segmentation into sentences
	\begin{center}
		\includegraphics[width=0.8\linewidth]{img/sentence_segmentation}
	\end{center}
	\item There are about 170'000 unique words in the English language at the moment
\end{itemize}

\subsubsection{Tokens and Types}
Tokens are the words on the page, while the type is the word forms. Counting the types requires lemmatisation, which is finding the lemma or base form for each word.

\subsubsection{Tokenisation and Normalisation}
Not as straightforward as one may think
\begin{itemize}
	\item punctuation
	\begin{itemize}
		\item periods and commas appear within words or abbreviations
		\item special tokens in mail addresses or tweets
		\item apostrophes
	\end{itemize}
	\item capital letters
	\item compound words
	\begin{itemize}
		\item problem complicated without dashes
		\item German words
		\item proper names
	\end{itemize}
\end{itemize}

\subsubsection{Sentence Segmentation}
Can be easier or more difficult depending on the source text formatting. If no particular information is available from the layout, punctuations and casing can be used. While question and exclamation marks are quite reliable indicator of sentences periods are not. A good approach is to try to combine Tokenisation and Sentence Splitting, focusing on full stops.

\section{Text Classification}
The goal of text classification is to assign text documents to one or more categories. There is a predefined set of classes, in which previously unseen documents are assigned to. If there are only two classes this is a binary classification problem.

To tackle this problem there are different strategies: (a) have hard-coded rules carefully crafted by an expert on the basis on combinations of words or other features, (b) through supervised machine learning with understanding building on semantic representation of texts and labels or (c) supervised machine learning without understanding where word-based features are derived from text and the relationship between features and labels from the training texts.

Stopwords are words like conjunctions or preposition, which may not have meaning given the task at hand.

\subsection{Formalising Text Classification}

\begin{itemize}[label=]
	\item Input
	\begin{itemize}
		\item a set of documents $D$
		\item a fixed set $N$ of classes $C$
		\item a training set of $M$ hand-labelled documents $(d_1 C_1),(d_2 C_2), \dots (d_M C_M)$
	\end{itemize}
	\item Output
	\begin{itemize}
		\item a mapping $D\rightarrow C$ that associates a predicted class $c\in C$ to each document $d\in D$
	\end{itemize}
\end{itemize}

\subsection{Naïve Bayes Classifier}
For a document $d$ and a class $c$

\begin{equation*}
	P(c|d) = \frac{P(d|c)P(c)}{P(d)}
\end{equation*}

\noindent
Maximum A Posteriori (MAP) classifier:
\begin{equation*}
	c_{\text{MAP}} = \underset{c\in C}{\text{argmax}} \frac{P(d|c) P(c)}{P(d)} = \underset{c\in C}{\text{argmax}} P(d|c) P(c)
\end{equation*}

Dropping the denominator does not change $c_{\text{MAP}}$ because $P(d)$ has no effect on argmax. In practice, the evidence a machine can observe is not the human-readable document $d$, but a number of features $x_1,\dots,x_N$ obtained based on $d$ and a MAP-classifier
\begin{equation*}
	c_MAP = \underset{c\in C}{\text{argmax}} P(x_1, x_2, \dots , x_N | c) P(c)
\end{equation*}

\noindent
Given a vocabulary of V words a feature can be if a word appears in a document $d$ or how often it appears
\begin{itemize}
	\item \textbf{Bernoulli Model} represent $d$ as $(e_1, \dots, e_V)$ where $e_i =1$ if the word $i$ is in $d$ and $e_i = 0$ otherwise
	\item \textbf{Multinomial Model} represent $d$ as $f_1, \dots, f_V)$ where $f_i$ is the number of occurrences of word $i$ in $d$
\end{itemize}

The Naïve Bayes independence assumption is that given a class $c$ the \textbf{features are independent}
\begin{equation*}
	P(x_1, x_2, \dots , x_N | c) = P(x_1|c)\cdot P(x_2|c) \cdots P(x_N|c) = \prod_{k=1}^{n}P(x_k|c)
\end{equation*}

$P(c)$ and $P(x_k|c)$ need to be computed, $P(c)$ can be estimated based on the frequency of each class in the training data, and $P(x_k|c)$ depends on the chosen feature representation.

In the so called Bag-Of-Word Model $x_k$ is the feature representation of the words in the documents. The position of the individual words can usually be ignored.

\subsection{Multinomial Naïve Bayes}
Represent every token in $d$ as a feature vector $x_i = f_i$ , where $f_i$ is the number of occurrences of token $i$ in $d$. Then $P(x_i|c)$ can be estimated as $\frac{\text{number of occurrences of token }i\text{ in }c}{\text{total number of tokens in }c}$. For simplicity $P(w|c)$ can be written to refer to the probability of finding token $w$ in class $c$.

\begin{enumerate}
	\item Normalise the training data (remove stop words, remove punctuation, set all characters to lowercase)
	\item Assemble vocabulary (list of unique meaningful words)
	\item Count the number of occurrences of each word in each class and divide by the total number of words in each class
\end{enumerate}

\subsection{Practical Complications}
If a word $w$ from the vocabulary (which contains $V$ tokens) is never in class $c$ in the training data, we will estimate $P(w|c) = 0$, which causes $c_{\text{corpus}}=\underset{c\in C}{\text{argmax}}P(c)\prod_{k=1}^{N}P(x_k|c)=0$.

\begin{remark}
	Zero probabilities cannot be conditioned away, no matter the other evidence!\\ - Dan Jurafsky
\end{remark}

The typical solution to this is to do Laplace Smoothing like this
\begin{equation*}
	P(w|c)\text{ can be estimated as }\frac{\text{number of occurrences of token $w$ in class } c + 1}{\text{total number of tokens in class } c+V}
\end{equation*}
This prevents probabilities of zero to occur.

\subsection{Metrics}
\begin{itemize}[leftmargin=*, labelindent=3cm, labelsep=1cm]
	\item[True Positive] Sample $c$ classified correctly
	\item[False Positive] Non-$c$ sample incorrectly classified as $c$
	\item[True Negative] Non-$c$ sample classified correctly
	\item[False Negative] Sample $c$ classified incorrectly as non-$c$
\end{itemize}

\begin{tabularx}{\linewidth}{rX}
	Accuracy & $ \frac{\sum TP}{\sum \text{all elements}} $ \\
	Precision & $\frac{TP}{TP + FP}$\\
	Recall & $ \frac{TP}{TP + FN} $\\
\end{tabularx}




\end{document}
